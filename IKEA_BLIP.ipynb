{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1n3-ntYEE9kyRDRPCnCBq3FolCk_2hxVV",
      "authorship_tag": "ABX9TyME3DShL2bGE2Bd1PERY5Xw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adithyan773/IKEA_recomendation_system/blob/main/IKEA_BLIP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwyZAvVbbpJV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from PIL import Image as PILImage\n",
        "from IPython.display import display\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "\n",
        "# Load the DataFrame\n",
        "df = pd.read_csv('/content/ikea_data_with_images.csv')\n",
        "df['item_id'] = df['item_id'].astype(str)\n",
        "df = df[df['item_id'] != 'nan']\n",
        "df = df.drop_duplicates(subset='item_id', keep='first')\n",
        "print(f\"Number of rows after removing duplicates: {len(df)}\")\n",
        "\n",
        "# Define image directory\n",
        "image_dir = '/content/drive/MyDrive/images/images'\n",
        "\n",
        "# Generate and filter image paths\n",
        "image_paths = [os.path.join(image_dir, f\"{row['item_id']}.jpg\") for _, row in df.iterrows()]\n",
        "valid_paths = [path for path in image_paths if os.path.exists(path)]\n",
        "print(f\"Number of valid image paths: {len(valid_paths)}\")\n",
        "\n",
        "# Filter DataFrame\n",
        "valid_item_ids = [os.path.basename(path).split('.')[0] for path in valid_paths]\n",
        "df = df[df['item_id'].isin(valid_item_ids)]\n",
        "print(f\"Number of rows after filtering by valid images: {len(df)}\")\n",
        "\n",
        "# Load BLIP Large model and processor\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Custom Dataset for batching\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, image_paths):\n",
        "        self.image_paths = image_paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.image_paths[idx]\n",
        "\n",
        "# Function to generate descriptions in batches\n",
        "def generate_image_descriptions_batch(image_paths_batch):\n",
        "    try:\n",
        "        images = [PILImage.open(path).convert(\"RGB\") for path in image_paths_batch]\n",
        "        inputs = processor(images, return_tensors=\"pt\", padding=True)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            out = model.generate(**inputs, max_length=50, num_beams=1)  # Faster with num_beams=1\n",
        "        descriptions = [processor.decode(o, skip_special_tokens=True) for o in out]\n",
        "        return descriptions\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing batch: {e}\")\n",
        "        return [\"\"] * len(image_paths_batch)\n",
        "\n",
        "# Create DataLoader for batch processing\n",
        "dataset = ImageDataset(valid_paths)\n",
        "batch_size = 16  # Adjust based on GPU memory (e.g., 16 for T4 with 15GB)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Generate descriptions\n",
        "image_descriptions = {}\n",
        "for batch_idx, batch_paths in enumerate(dataloader):\n",
        "    print(f\"Processing batch {batch_idx + 1}/{len(dataloader)}\")\n",
        "    descriptions = generate_image_descriptions_batch(batch_paths)\n",
        "    for path, desc in zip(batch_paths, descriptions):\n",
        "        item_id = os.path.basename(path).split('.')[0]\n",
        "        image_descriptions[item_id] = desc\n",
        "\n",
        "# Add descriptions to DataFrame\n",
        "df['image_description'] = df['item_id'].map(image_descriptions)\n",
        "\n",
        "# Validation: Display first 5 images and descriptions\n",
        "print(\"\\nValidation: Generated Image Descriptions\")\n",
        "for i in range(min(5, len(df))):\n",
        "    row = df.iloc[i]\n",
        "    item_id = row['item_id']\n",
        "    path = os.path.join(image_dir, f\"{item_id}.jpg\")\n",
        "    description = row['image_description']\n",
        "    print(f\"Item ID: {item_id}\")\n",
        "    print(f\"Description: {description}\")\n",
        "    display(PILImage.open(path))\n",
        "\n",
        "# Save updated DataFrame\n",
        "output_csv = '/content/ikea_data_img_fixed.csv'\n",
        "df.to_csv(output_csv, index=False)\n",
        "print(f\"Updated dataset saved to: {output_csv}\")"
      ]
    }
  ]
}